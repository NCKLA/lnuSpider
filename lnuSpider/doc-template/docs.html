<!DOCTYPE html>
<html>
<head>
<script type="text/javascript">
     SyntaxHighlighter.all();
</script>
<meta charset="utf-8">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
<title>LnuSpider帮助文档Demo</title>
<link rel="alternate" type="application/rss+xml" title="egrappler.com" href="feed/index.html">
<link href="http://fonts.googleapis.com/css?family=Raleway:700,300" rel="stylesheet"
        type="text/css">
<link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/prettify.css">
</head>
<body>
<nav>
  <div class="container">
    <h1>空的，还没写 甚至可能删除</h1>
    <div id="menu">
      <ul class="toplinks">
        <li><a href="#">别点 点了也没用</a></li>
      </ul>
    </div>
    <a id="menu-toggle" href="#" class=" ">&#9776;</a> </div>
</nav>
<header>
  <div class="container">
    <h2 class="docs-header">LnuSpider 帮助文档</h2>
  </div>
</header>
<section>
  <div class="container">
    <ul class="docs-nav" id="menu-left">
      <li><strong>scrapy爬虫基本操作介绍</strong></li>
      <li><a href="#info" class=" ">info</a></li>
      <li class="separator"></li>
      <li><strong>进阶操作</strong></li>
      <li><a href="#fanye" class=" ">爬虫翻页</a></li>
      <li><a href="#in" class=" ">爬取内页</a></li>
      <li><a href="#redis" class=" ">连接redis数据库</a></li>
      <li><a href="#dongtai" class=" ">动态网页无法获取数据解决办法</a></li>

      <li class="separator"></li>
      <li><strong>爬取到的数据信息</strong></li>
      <li><a href="#souhuhao_sohucaijing" class=" ">搜狐号 搜狐财经</a></li>
      <li><a href="#" class=" "></a></li>
      <li class="separator"></li>
      <li><strong>文档编写部分</strong></li>
      <li><a href="#write_doc" class=" ">文档编写</a></li>
      <li><a href="#write_left_bar" class=" ">侧边栏档编写</a></li>
    </ul>
    <div class="docs-content">
      <h2> 准备好踩爬虫坑了吗？</h2>
      <h3 id="info"> info</h3>
      <p> 很显然，这里什么都没准备写</p>
      
      <p> 真的没写</p>

        <hr>
      <h2> 进阶操作</h2>
      <h3 id="fanye">爬虫翻页</h3>
      <ul>
        <li>代码实现</li>
      </ul>
      <pre class="prettyprint">
             print("========准备翻页========")
             next_url = response.xpath("//span[@class='num-container']/a[last()]/@href").getall()
             next_url = "".join(next_url)
             if not next_url:
                 print("===结束===")
             return
                 else:
             yield scrapy.Request(next_url, callback=self.parse)
             print("=====翻页成功======")
      </pre>
        <ul>
        <li>效果图展示</li>
        </ul>
        <img src="pic/fanye.png" alt="fanye.png">
        <h3 id="in">爬取内页</h3>
        <ul>
        <li>代码实现</li>
      </ul>
      <pre class="prettyprint">
             （1） 获取内页的url以及当前页所得到的数据，并跳转到detail函数
                 yield scrapy.Request(item['url'], meta={'item': item}, callback=self.detail)
             （2）  detail函数代码，获取内页中所需数据：
 	             def detail(self, response):
      		           # 接收上级已爬取的数据
      		               print("========已经进入内页=========")
       		               item = response.meta['item']
                       # 一级内页数据提取
                           item['date'] = "".join(item['date']).strip()
       			           item['cont'] = response.xpath("//div[@class='main-text atc-content']/p/text()").getall()
      			           item['cont'] = "".join(item['cont']).strip()
        		           yield item
      </pre>
        <ul>
        <li>效果图展示</li>
        </ul>
        <img src="pic/in.png" alt="in.png">
        <h3 id="redis">连接redis数据库部分操作</h3>
        <ul>
        <li>修改配置文件代码实现</li>
      </ul>
      <pre class="prettyprint">
             # 1. 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化
                    DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
             # 2. 增加了调度的配置, 作用: 把请求对象存储到Redis数据, 从而实现请求的持久化.
                    SCHEDULER = "scrapy_redis.scheduler.Scheduler"
             # 3. 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set
             # 如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据
                    SCHEDULER_PERSIST = True
             # 4 方式:
             # REDIS_HOST = '127.0.0.1'
             # REDIS_PORT = 6379
             # 5. 如果需要把数据存储到Redis数据库中, 可以配置RedisPipeline
                    ITEM_PIPELINES = {
                                          # 把爬虫爬取的数据存储到Redis数据库中
                                          'scrapy_redis.pipelines.RedisPipeline': 400,
                                       }
      </pre>
        <ul>
        <li>修改spider.py代码实现</li>
      </ul>
      <pre class="prettyprint">
             # -*- coding: utf-8 -*-
                    import random
                    import time
                    import scrapy
                    from jqka.items import JqkaItem
                    from scrapy import Request
                    from scrapy.http.response.html import HtmlResponse
                    from scrapy.selector.unified import SelectorList
                    from scrapy_redis.spiders import RedisSpider
                    class JqkaSpiderSpider(RedisSpider):
                            name = 'jqka_spider'
                            # allowed_domains = ['news.10jqka.com.cn']
                            # 分布式需要删掉增加redis_key
                            # start_urls = ['http://news.10jqka.com.cn/today_list/index_2.shtml']
                            redis_key = "Jqka"
      </pre>
         <ul>
        <li>效果图展示</li>
        </ul>
        <img src="pic/redis.png" alt="redis.png">

        <h3 id="dongtai">动态网页无法获取数据解决办法</h3>
        <ul>
        <li>适用于scrapy shell无法获取，F12也看不到xhr请求，但是查看元素存在的情况</li>
        </ul>
        <p>
            最简单的办法，直接换上我的中间件dongtaiMiddleware，这个中间件会模拟从长到下6次均匀网页的操作，并且每次滚动都会停一秒。
        </p>
        <ul>
        <li>操作过程</li>
        </ul>
        <p>
            lnuSpider下有个phantomj.exe,复制并放到相应路径。
            max linux必须在/usr/local/bin   ，win必须在c盘，建议在c盘的用户文件目录下建一个/local/bin再把东西放进去。
        </p>
        <p>
            python，pip安装selenium，中间件导入from selenium import webdriver。之后直接复制middlewares.py的dongtaiMiddleware，
            这个class的源码到你自己的中间件（就是说不能直接在源代码上改，改个class名原地复制都行）。
        </p>
        <p>
            修改你的settings.py
        </p>
        <pre class="prettyprint">

            DOWNLOADER_MIDDLEWARES = {
               'lnuSpider.middlewares.SeleniumSpiderMiddleware': 543,
            }

        </pre>
        <p>
            修改你复制来的中间件的部分信息
        </p>
        <pre class="prettyprint">

    ......

    class dongtaiMiddleware(object):
        def __init__(self):
            # 配置你的路径 max linux必须在/usr/local/bin   win必须在c盘，建议在c盘的用户文件目录下建一个/local/bin再把东西放进去
            self.driver = webdriver.PhantomJS(executable_path=r'C:\Users\G50\local\bin\phantomjs.exe')

        def process_request(self, request, spider):
            # 这里的爬虫名换成你自己的
            if spider.name == 'sohucaijing_Spider':

    ......
        </pre>



        <hr>
        <h2> 爬取到的数据格式</h2>
        <h3 id="souhuhao_sohucaijing">搜狐号 搜狐财经</h3>
        <h4>简介：</h4>
        <p>
            爬取自搜狐的搜狐号（类似微信公众号）中名为“搜狐财经”的官方账号。特点是初始页面通过滚动到页面底部会产生ajax请求。
            而内页的评论部分初次访问爬取不到信息，需要进行多次访问或者是模拟滚动操作。
        </p>
        <h4>基本信息：</h4>
        <ul>
            <li>起始链接：<a href="http://mp.sohu.com/profile?xpt=c29odWNqeWMyMDE3QHNvaHUuY29t&_'
                  'f=index_pagemp_1&spm=smpc.ch15.top-subnav.8.1585379351817DgmoPb1/">搜狐号_搜狐财经</a></li>
            <li>爬虫文件:  sohucaijing_Spider.py</li>
            <li>pipeline所用class:  LnuspiderPipeline</li>
            <li>middlewaress所用class:  SeleniumSpiderMiddleware</li>
            <li>items所用class:  LnuspiderItem</li>
            <li>图片文件夹:  sohucaijing</li>
            <li>json文件前缀:  搜狐号_搜狐财经</li>
        </ul>
        <h4>数据格式及内容：</h4>
        <ul>
            <li>title:  string 新闻的标题</li>
            <li>content:  string 新闻的正文内容。其中，图片部分的格式为  (图片:xxxxxxxx.jpeg)
            括号和冒号均为英文符号。正文中黑体字标题部分获取了其文字并在尾部添加了一个句号</li>
            <li>tags:  list(string) 新闻的标题，数量0-3不等，大多为3个</li>
            <li>date:  string 日期 ，格式为 年-月-日 时:分</li>
            <li>url:  string 子页面链接，也就是单个新闻的链接</li>
            <li>images_src:  list(string) 图片的原链接，按照爬取的顺序存放，0到多个，一般不超过十个</li>
            <li>comments:  list(object/dict) 评论信息，单个评论信息的数组，数量最小为0最大还不知道有多少</li>
            <ul>
                <a>单个评论内容</a>
                <li>username:  string 用户名</li>
                <li>location:  string 所在地，格式为某省某市，由一对英文括号包住</li>
                <li>date:  string 评论日期，x月x日 xx:xx，需要注意的是，与当前时间间隔在一小时以内时会变成相对时间，比如：15分钟前【*有可能为空】</li>
                <li>discuss:  string 评论内容 可能会有emoji之类的特殊字符【*有可能为空】</li>
                <li>thumb:  int 点赞数【*有可能为空】</li>
            </ul>
        </ul>

        <hr>
      <h2> 文档编写部分</h2>
        <h3 id="write_doc">文档编写</h3>
        <p>
            文档的编写比较简单，直接使用&lt;p&gt;标签，在标签内写入文本，若需要换行则可以再加上一个&lt;p&gt;标签。
        </p>
      <ul>
        <li>如果你想要这种前面带个点的标题效果，请使用&lt;ul&gt; &lt;li&gt;标签</li>
      </ul>
        <pre class="prettyprint">
           如果你需要插入一段代码 则需要使用&lt;pre class="prettyprint"&gt;标签
           虽然不保证跟ide一样的识别度。但是变色高亮总是让人看的舒服。下面是一段代码示例

            =============================================
            def __init__(self):
            ssstime = time.strftime("%Y-%m-%d %H-%M-%S", time.localtime())
            self.fp = open("lnuSpider/data/json/搜狐号_搜狐财经_"+ssstime+".json", 'wb')
            self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=False)
            # self.http = urllib3.PoolManager()
        </pre>
        <p>
            值得注意的事 前面您所看到的标签，虽然有着正确的写法，但是却没有发挥其再html中应有的效果。
            那是因为在源码编写时用了 & it;   和   & gt;  来表示一对尖括号
        </p>
        <p>
            添加图片的话放到pic文件夹即可
        </p>
        <h3 id="write_left_bar">侧边栏编写</h3>
        <p>
            首先需要写出一个h3的标题标签，比如&lt;h3 id="write_left_bar"&gt;侧边栏编写&lt;/h3&gt;
            , id如果需要的话记得加下划线。其次需要在&ltul class="docs-nav" id="menu-left"&gt;中加入一条li标签，
            id格式需要对应，记得加#号。
        </p>
        <p>
            有些时候刚打开页面未加载完全时侧边栏不会跟随浏览一起移动，或者代码段没有高亮变色，此时等待网页加载完全即可。
        </p>
    </div>
  </div>
</section>
<section class="vibrant centered">
  <div class="container">
    <h4> 本文档尚在编写中。。。 <a href="#"> here</a></h4>
  </div>
</section>
<footer>

</footer>

<script src="js/jquery.min.js"></script>

<script type="text/javascript" src="js/prettify/prettify.js"></script> 
<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js?lang=css&skin=sunburst"></script>
<script src="js/layout.js"></script>
 <script src="js/jquery.localscroll-1.2.7.js" type="text/javascript"></script>
 <script src="js/jquery.scrollTo-1.4.3.1.js" type="text/javascript"></script>
</body>
</html>
