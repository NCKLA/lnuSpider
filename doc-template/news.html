<!DOCTYPE html>
<html>
<head>
<script type="text/javascript">
     SyntaxHighlighter.all();
</script>
<meta charset="utf-8">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
<title>News Description</title>
<link rel="stylesheet" href="css/style.css">
<link rel="stylesheet" href="css/prettify.css">
</head>
<body>

<header>
  <div class="container">
    <h2 class="docs-header">相关新闻数据描述文档</h2>
  </div>
</header>
<section>
  <div class="container">
    <ul class="docs-nav" id="menu-left">
      <li><strong>新闻基本介绍</strong></li>
      <li><a href="#info" class=" ">基本介绍</a></li>
      <li class="separator"></li>

      <li class="separator"></li>
      <li><strong>爬取到的数据信息</strong></li>
      <li><a href="#souhuhao_sohucaijing" class=" ">和讯网</a></li>
      <li><a href="#nfcf_caijingyaowen" class=" ">南方财富网 财经要闻</a></li>
      <li><a href="#souhuhao_sohucaijing" class=" ">搜狐号 搜狐财经</a></li>
      <li><a href="#" class=" "></a></li>
      <li class="separator"></li>

    </ul>
    <div class="docs-content">
      <h2> 基本介绍</h2>
      <p>
          此文档主要是关于所有新闻数据的描述，包括数据的来源网址，数据的格式，数据的大小等等信息
      </p>

        <h2> 爬取到的数据信息</h2>
        <h3 id="hexundujia">和讯网 和讯独家</h3>
        <h4>简介：</h4>
        <p>
            和讯独家的内容，一条新闻就是一篇文章。
        </p>
        <h4>基本信息：</h4>
        <ul>
            <li>起始链接：<a href="http://news.hexun.com/original/">和讯独家</a></li>
            <li>爬虫文件:  hexunspider.py以及hexun_hyz.py</li>
            <li>hexunspider.py这个文件主要是用于文章网址的提取</li>
            <li>hexun_hyz.py这个文件主要是爬取获取下来的文章的网址</li>
            <li>pipelines用到的class：HexunPipeline以及HexunPipeline2</li>
            <li>因为有两个爬虫文件，所以要给他们指定不同的pipeline</li>
            <li>middlewaress所用class:  HexunSpiderMiddleware</li>
            <li>items所用class:  HexunItem以及Hexun2Item</li>
            <li>HexunItem用于生成文章网址，Hexun2Item用于生成文章详情</li>
            <li>图片文件夹:  D:\PythonProject\hexun\hexun\data\image\hexun2</li>
            <li>json文件前缀:  文章详情</li>
        </ul>
        <h4>数据格式及内容：</h4>
        <ul>
            <li>每条数据包括六个部分：文章链接url、文章标题title、文章发布时间time、文章内容content、图片列表image_src、下载下来的图片</li>
            <li>url：这个是通过meta直接从文章网址里面获取过来</li>
            <li>title：string类型的文章标题</li>
            <li>time：string类型的数据，文章的发表时间，格式为：xxxx-xx-xx xx:xx:xx</li>
            <li>content：string类型的新闻内容，在文章盒子的p标签下</li>
            <li>image_src：list类型的数据，将一篇文章里面所有的图片链接保存到列表里面</li>
            <li>下载下来的图片统一放到一个本地的文件夹中</li>

            </ul>
        </ul>
        <h4>获取下来的数据大小的统计记录</h4>
        <ul>
            <li>统计时间：2020年5月12号</li>
            <li>计划统计范围：爬取到的所有的数据大小</li>
            <li>实际统计数量：2203条数据</li>
            <li>数据丢失：获取下来的文章url大概是2218，但是最终获取到的数据只有2203条，有十几篇文章出现了数据丢失的情况</li>
            <li>数据总大小：380MB图片数据+6.64MB文本数据</li>
            <li>数据平均大小：图片17.1MB/100条数据，文字0.3MB/100条数据</li>
            <li>时间最近数据：2020.05.03</li>
            <li>时间最远数据：2020.01.07</li>
            <li>和讯网的数据大概四个月更新完100页，所以可以大概三个半月再爬一遍</li>
            <li>预计下次启动时间：2020.8.15左右li>
        </ul>
        <hr>




        <h3 id="nfcf_caijingyaowen">南方财富网 财经要闻</h3>
        <h4>简介：</h4>
        <p>
            爬取南方财富网财经要闻模块，采用crawlspider类，可以定义爬取的url的规则，以后scrapy碰到满足条件的url都进行爬取，
            不用手动yield request。其中包含LinkExtractors连接提取器、Rule规则类。新闻列表需要翻页，列表中每条新闻的内容需要翻页，
            所以需要两层跟进。
        </p>
        <h4>基本信息：</h4>
        <ul>
            <li>起始链接：<a href="http://www.southmoney.com/caijing/caijingyaowen/">南方财富网_财经要闻</a></li>
            <li>爬虫文件:  nfcf_spider.py</li>
            <li>pipeline所用class:  NfcjPipeline</li>
            <li>middlewaress所用class:  NfcjSpiderMiddleware</li>
            <li>items所用class:  NfcjItem</li>
            <li>json文件前缀:  nfcf</li>
        </ul>
        <h4>数据格式及内容：</h4>
        <ul>
            <li>每条数据包括三个部分：文章标题title、文章发布时间及来源date_source、文章内容article</li>
            <li>title：string类型 文章标题</li>
            <li>date_source：string类型 文章的发表时间及来源，格式为：xxxx-xx-xx xx:xx xx 例：2020-04-22 14:02\n互联网\n</li>
            <li>content：string类型 新闻内容</li>
        </ul>
        <h4>数据大小统计记录（可以写多个）</h4>
        <ul>
            <li>统计时间：2020年5月13号</li>
            <li>计划统计范围：爬取到的所有的数据大小</li>
            <li>实际统计数量：2511条数据</li>
            <li>数据丢失：获取下来的文章是2511，但是最终获取到的数据大概2300条</li>
            <li>数据总大小：5.12MB文本数据</li>
            <li>数据平均大小：文字0.21MB/100条数据</li>
            <li>时间最近数据：2020.04.30</li>
            <li>时间最远数据：2013.06.08</li>
            <li>南方财富网的数据大概每天更新70条，统计周期三个月</li>
            <li>预计下次启动时间：2020.8.13</li>
        </ul>
        <hr>

        <h3 id="souhuhao_sohucaijing">搜狐号 搜狐财经</h3>
        <h4>简介：</h4>
        <p>
            爬取自搜狐的搜狐号（类似微信公众号）中名为“搜狐财经”的官方账号。特点是初始页面通过滚动到页面底部会产生ajax请求。
            而内页的评论部分初次访问爬取不到信息，需要进行多次访问或者是模拟滚动操作。
        </p>
        <h4>基本信息：</h4>
        <ul>
            <li>起始链接：<a href="http://mp.sohu.com/profile?xpt=c29odWNqeWMyMDE3QHNvaHUuY29t&_'
                  'f=index_pagemp_1&spm=smpc.ch15.top-subnav.8.1585379351817DgmoPb1/">搜狐号_搜狐财经</a></li>
            <li>爬虫文件:  sohucaijing_Spider.py</li>
            <li>pipeline所用class:  LnuspiderPipeline</li>
            <li>middlewaress所用class:  SeleniumSpiderMiddleware</li>
            <li>items所用class:  LnuspiderItem</li>
            <li>图片文件夹:  sohucaijing</li>
            <li>json文件前缀:  搜狐号_搜狐财经</li>
        </ul>
        <h4>数据格式及内容：</h4>
        <ul>
            <li>title:  string 新闻的标题</li>
            <li>content:  string 新闻的正文内容。其中，图片部分的格式为  (图片:xxxxxxxx.jpeg)
            括号和冒号均为英文符号。正文中黑体字标题部分获取了其文字并在尾部添加了一个句号</li>
            <li>tags:  list(string) 新闻的标题，数量0-3不等，大多为3个</li>
            <li>date:  string 日期 ，格式为 年-月-日 时:分</li>
            <li>url:  string 子页面链接，也就是单个新闻的链接</li>
            <li>images_src:  list(string) 图片的原链接，按照爬取的顺序存放，0到多个，一般不超过十个</li>
            <li>comments:  list(object/dict) 评论信息，单个评论信息的数组，数量最小为0最大还不知道有多少</li>
            <ul>
                <p>单个评论内容</p>
                <li>username:  string 用户名</li>
                <li>location:  string 所在地，格式为某省某市，由一对英文括号包住</li>
                <li>date:  string 评论日期，x月x日 xx:xx，需要注意的是，与当前时间间隔在一小时以内时会变成相对时间，比如：15分钟前【有可能为空】</li>
                <li>discuss:  string 评论内容 可能会有emoji之类的特殊字符【有可能为空】</li>
                <li>thumb:  int 点赞数【有可能为空】</li>
            </ul>
        </ul>
        <h4>数据大小统计记录（可以写多个）</h4>
        <ul>
            <li>统计时间：2020.5.11</li>
            <li>计划统计范围：统计时间下前1000条新闻</li>
            <li>实际统计数量：468条（爬虫在第721条新闻处结束爬取）</li>
            <li>数据总大小：213MB(图片)+2.36MB(文字，评论爬取不完全)</li>
            <li>数据平均大小：文字0.5MB/100条，图片45.51MB/100条</li>
            <li>时间最远数据：2020.2.18</li>
            <li>预计下次统计时间：2020.7.1</li>
        </ul>
        <ul>
            <li>统计时间：2020.5.12</li>
            <li>计划统计范围：统计时间下前1000条新闻</li>
            <li>实际统计数量：536条（爬虫在第981条新闻处结束爬取）</li>
            <li>数据总大小：247MB(图片)+2.85MB(文字，评论爬取不完全)</li>
            <li>数据平均大小：文字0.53MB/100条，图片46.09MB/100条</li>
            <li>时间最远数据：2019.12.30</li>
            <li>预计下次统计时间：2020.7.1</li>
        </ul>
        <hr>

    </div>
  </div>
</body>
</html>
